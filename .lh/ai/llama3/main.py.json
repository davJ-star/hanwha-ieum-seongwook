{
    "sourceFile": "ai/llama3/main.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1737029739627,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1737029739627,
            "name": "Commit-0",
            "content": "from fastapi import FastAPI\r\nfrom pydantic import BaseModel\r\nimport httpx\r\n\r\napp = FastAPI()\r\n\r\nclass LlamaRequest(BaseModel):\r\n    model: str\r\n    prompt: str\r\n\r\n@app.post(\"/generate\")\r\nasync def generate_response(request: LlamaRequest):\r\n    url = \"http://localhost:11434/api/generate\"  # Adjust based on your LLaMA 3 API endpoint\r\n    headers = {'Content-Type': 'application/json'}\r\n    \r\n    async with httpx.AsyncClient() as client:\r\n        response = await client.post(url, json=request.dict(), headers=headers)\r\n        \r\n    if response.status_code == 200:\r\n        return response.json()\r\n    else:\r\n        return {\"error\": \"Failed to generate response\", \"status_code\": response.status_code}\r\n\r\n@app.get(\"/\")\r\ndef read_root():\r\n    return {\"message\": \"Welcome to the LLaMA 3 FastAPI service!\"}\r\n"
        }
    ]
}