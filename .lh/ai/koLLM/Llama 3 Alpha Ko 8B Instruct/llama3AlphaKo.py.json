{
    "sourceFile": "ai/koLLM/Llama 3 Alpha Ko 8B Instruct/llama3AlphaKo.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1737099989501,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1737099989501,
            "name": "Commit-0",
            "content": "import torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\ndef load_model():\r\n    model_name = \"beomi/llama-3-ko-8b-instruct\"\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\r\n    return tokenizer, model\r\n\r\ndef generate_response(tokenizer, model, prompt, max_length=512):\r\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\r\n    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1, do_sample=True, temperature=0.7)\r\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\r\n    return response\r\n\r\ndef main():\r\n    tokenizer, model = load_model()\r\n    \r\n    while True:\r\n        user_input = input(\"사용자: \")\r\n        if user_input.lower() == 'quit':\r\n            break\r\n        \r\n        prompt = f\"사용자: {user_input}\\n시스템: 사용자의 질문에 친절하고 정확하게 답변해주세요.\\n답변: \"\r\n        response = generate_response(tokenizer, model, prompt)\r\n        print(\"모델:\", response)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n"
        }
    ]
}