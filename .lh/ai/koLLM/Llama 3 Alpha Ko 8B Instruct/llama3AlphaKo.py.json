{
    "sourceFile": "ai/koLLM/Llama 3 Alpha Ko 8B Instruct/llama3AlphaKo.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1737099989501,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1737101840968,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,9 +2,9 @@\n from transformers import AutoTokenizer, AutoModelForCausalLM\r\n \r\n def load_model():\r\n     model_name = \"beomi/llama-3-ko-8b-instruct\"\r\n-    tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n+    tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\r\n     model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\r\n     return tokenizer, model\r\n \r\n def generate_response(tokenizer, model, prompt, max_length=512):\r\n"
                },
                {
                    "date": 1737101853579,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n \r\n def load_model():\r\n     model_name = \"beomi/llama-3-ko-8b-instruct\"\r\n     tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\r\n-    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\r\n+    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\r\n     return tokenizer, model\r\n \r\n def generate_response(tokenizer, model, prompt, max_length=512):\r\n     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\r\n"
                },
                {
                    "date": 1737101912544,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,6 +1,7 @@\n import torch\r\n from transformers import AutoTokenizer, AutoModelForCausalLM\r\n+import os\r\n \r\n def load_model():\r\n     model_name = \"beomi/llama-3-ko-8b-instruct\"\r\n     tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\r\n"
                }
            ],
            "date": 1737099989501,
            "name": "Commit-0",
            "content": "import torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\n\r\ndef load_model():\r\n    model_name = \"beomi/llama-3-ko-8b-instruct\"\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\r\n    return tokenizer, model\r\n\r\ndef generate_response(tokenizer, model, prompt, max_length=512):\r\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\r\n    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1, do_sample=True, temperature=0.7)\r\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\r\n    return response\r\n\r\ndef main():\r\n    tokenizer, model = load_model()\r\n    \r\n    while True:\r\n        user_input = input(\"사용자: \")\r\n        if user_input.lower() == 'quit':\r\n            break\r\n        \r\n        prompt = f\"사용자: {user_input}\\n시스템: 사용자의 질문에 친절하고 정확하게 답변해주세요.\\n답변: \"\r\n        response = generate_response(tokenizer, model, prompt)\r\n        print(\"모델:\", response)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n"
        }
    ]
}