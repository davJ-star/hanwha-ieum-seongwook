{
    "sourceFile": "ai/llamaAPI/main.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1737031186900,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1737032766159,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,48 +1,71 @@\n-from fastapi import FastAPI\r\n-import requests\r\n+from fastapi import FastAPI, HTTPException\r\n from pydantic import BaseModel\r\n+import httpx\r\n+import logging\r\n \r\n app = FastAPI()\r\n \r\n-# Define a request model for the prompt input\r\n+# 로깅 설정\r\n+logging.basicConfig(level=logging.INFO)\r\n+logger = logging.getLogger(__name__)\r\n+\r\n+# 요청 모델 정의\r\n class PromptRequest(BaseModel):\r\n     model: str\r\n     prompt: str\r\n \r\n-# Endpoint to generate text using Llama3 API\r\n+# Llama3 API URL\r\n+LLAMA3_API_URL = \"http://localhost:11434/api\"\r\n+\r\n+# /generate 엔드포인트\r\n @app.post(\"/generate\")\r\n async def generate_text(request: PromptRequest):\r\n-    url = \"http://localhost:11434/api/generate\"  # Replace with your Llama3 API URL\r\n+    logger.info(f\"Received generate request: {request}\")\r\n+    url = f\"{LLAMA3_API_URL}/generate\"\r\n     headers = {'Content-Type': 'application/json'}\r\n     \r\n     data = {\r\n         \"model\": request.model,\r\n         \"prompt\": request.prompt\r\n     }\r\n     \r\n-    response = requests.post(url, json=data, headers=headers)\r\n-    \r\n-    if response.status_code == 200:\r\n-        return response.json()\r\n-    else:\r\n-        return {\"error\": f\"Request failed with status code: {response.status_code}\"}\r\n+    try:\r\n+        async with httpx.AsyncClient() as client:\r\n+            response = await client.post(url, json=data, headers=headers)\r\n+        \r\n+        if response.status_code == 200:\r\n+            return response.json()\r\n+        else:\r\n+            raise HTTPException(status_code=response.status_code, detail=\"Generate request failed\")\r\n+    except Exception as e:\r\n+        logger.error(f\"Generate request failed: {str(e)}\")\r\n+        raise HTTPException(status_code=500, detail=f\"Generate request failed: {str(e)}\")\r\n \r\n-# Endpoint to chat with Llama3 API\r\n+# /chat 엔드포인트\r\n @app.post(\"/chat\")\r\n async def chat_with_llama(request: PromptRequest):\r\n-    url = \"http://localhost:11434/api/chat\"  # Replace with your Llama3 API URL\r\n+    logger.info(f\"Received chat request: {request}\")\r\n+    url = f\"{LLAMA3_API_URL}/chat\"\r\n     headers = {'Content-Type': 'application/json'}\r\n     \r\n     data = {\r\n         \"model\": request.model,\r\n         \"messages\": [{\"role\": \"user\", \"content\": request.prompt}]\r\n     }\r\n     \r\n-    response = requests.post(url, json=data, headers=headers)\r\n-    \r\n-    if response.status_code == 200:\r\n-        return response.json()\r\n-    else:\r\n-        return {\"error\": f\"Request failed with status code: {response.status_code}\"}\r\n+    try:\r\n+        async with httpx.AsyncClient() as client:\r\n+            response = await client.post(url, json=data, headers=headers)\r\n+        \r\n+        if response.status_code == 200:\r\n+            return response.json()\r\n+        else:\r\n+            raise HTTPException(status_code=response.status_code, detail=\"Chat request failed\")\r\n+    except Exception as e:\r\n+        logger.error(f\"Chat request failed: {str(e)}\")\r\n+        raise HTTPException(status_code=500, detail=f\"Chat request failed: {str(e)}\")\r\n \r\n-# Run the application with 'uvicorn filename:app --reload'\r\n+# 서버 실행 (uvicorn main:app --reload)\r\n+if __name__ == \"__main__\":\r\n+    import uvicorn\r\n+    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\r\n"
                },
                {
                    "date": 1737033087064,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,8 +41,10 @@\n         logger.error(f\"Generate request failed: {str(e)}\")\r\n         raise HTTPException(status_code=500, detail=f\"Generate request failed: {str(e)}\")\r\n \r\n # /chat 엔드포인트\r\n+\r\n+# chat은 계속 오류가 남. 확인 바람람\r\n @app.post(\"/chat\")\r\n async def chat_with_llama(request: PromptRequest):\r\n     logger.info(f\"Received chat request: {request}\")\r\n     url = f\"{LLAMA3_API_URL}/chat\"\r\n@@ -68,4 +70,5 @@\n # 서버 실행 (uvicorn main:app --reload)\r\n if __name__ == \"__main__\":\r\n     import uvicorn\r\n     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\r\n+\r\n"
                }
            ],
            "date": 1737031186900,
            "name": "Commit-0",
            "content": "from fastapi import FastAPI\r\nimport requests\r\nfrom pydantic import BaseModel\r\n\r\napp = FastAPI()\r\n\r\n# Define a request model for the prompt input\r\nclass PromptRequest(BaseModel):\r\n    model: str\r\n    prompt: str\r\n\r\n# Endpoint to generate text using Llama3 API\r\n@app.post(\"/generate\")\r\nasync def generate_text(request: PromptRequest):\r\n    url = \"http://localhost:11434/api/generate\"  # Replace with your Llama3 API URL\r\n    headers = {'Content-Type': 'application/json'}\r\n    \r\n    data = {\r\n        \"model\": request.model,\r\n        \"prompt\": request.prompt\r\n    }\r\n    \r\n    response = requests.post(url, json=data, headers=headers)\r\n    \r\n    if response.status_code == 200:\r\n        return response.json()\r\n    else:\r\n        return {\"error\": f\"Request failed with status code: {response.status_code}\"}\r\n\r\n# Endpoint to chat with Llama3 API\r\n@app.post(\"/chat\")\r\nasync def chat_with_llama(request: PromptRequest):\r\n    url = \"http://localhost:11434/api/chat\"  # Replace with your Llama3 API URL\r\n    headers = {'Content-Type': 'application/json'}\r\n    \r\n    data = {\r\n        \"model\": request.model,\r\n        \"messages\": [{\"role\": \"user\", \"content\": request.prompt}]\r\n    }\r\n    \r\n    response = requests.post(url, json=data, headers=headers)\r\n    \r\n    if response.status_code == 200:\r\n        return response.json()\r\n    else:\r\n        return {\"error\": f\"Request failed with status code: {response.status_code}\"}\r\n\r\n# Run the application with 'uvicorn filename:app --reload'\r\n"
        }
    ]
}